---
layout: article
title: 决策树
tags: 
- Python
- Sklearn
- 菜菜
- 机器学习
toc: true
mathjax: true
mathjax_autoNumber: false
key: 20220214
---

## 概述

### 决策什么是决策树

决策树（Decision Tree）是一种非参数的**有监督**学习方法，它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。

<div align="center">
	<img src="https://raw.githubusercontent.com/cocotwp/cocotwp.github.io/master/assets/images/决策树/决策树样例.png" alt="决策树样例" width="60%" />
</div>

决策树算法的两个核心问题：
1. 如何从数据表中找出最佳节点和最佳分枝？
2. 如何让决策树停止胜场，防止过拟合？

### sklearn 中的决策树

**`sklearn.tree` 模块**

该模块包含以下类：
| tree.BaseDecisionTree | 基本决策树 |
| tree.DecisionTreeClassifier | 分类树 |
| tree.DecisionTreeRegressor | 回归树 |
| tree.ExtraTreeClassifier | 高随机版本的分类树 |
| tree.ExtraTreeRegressor | 高随机版本的回归树 | 
| tree.export_text 、 tree.export_graphviz 、 tree.plot_tree | 绘图 |

**基本建模流程**

```python
from sklearn import tree

# 1. 实例化模型对象
clf = tree.DecisionTreeClassifier()
# 2. 用训练集数据训练模型
clf = clf.fit(x_train, y_train)
# 3. 验证测试集
result = clf.score(x_test, y_test)
```

## DecisionTreeClassifier

_class_ sklearn.tree.DecisionTreeClassifier(_*_, _criterion='gini'_, _splitter='best'_, _max_depth=None_, _min_samples_split=2_, _min_samples_leaf=1_, _min_weight_fraction_leaf=0.0_, _max_features=None_, _random_state=None_, _max_leaf_nodes=None_, _min_impurity_decrease=0.0_, _class_weight=None_, _ccp_alpha=0.0_)

### 重要参数

#### criterion{“gini”, “entropy”}, default=”gini”

为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标叫做“不纯度”。现在使用的决策树算法在分枝方法上的核心大多是围绕在某个不纯度相关指标的最优化上。

sklearn 提供了两种选择：**信息熵**（Entropy）、**基尼系数**（Gini Impurity）

$$Entropy(t)=-\sum_{i=0}^{c-1}p(i|t)log_2p(i|t)$$
$$Gini(t)=1-\sum_{i=0}^{c-1}p(i|t)^2$$

其中 $t$ 代表给定的节点，$i$ 代表标签的任意分类，$p(i|t)$ 代表标签分类 $i$ 在节点 $t$ 上所占的比例。**注意**，当使用信息熵时，sklearn 实际计算的是基于信息熵的信息增益（Information Gain），即父节点的信息熵和子节点的信息熵之差。

比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是*在实际使用中，信息熵和基尼系数的效果基本相同*。

