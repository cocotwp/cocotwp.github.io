---
layout: article
title: 决策树
tags: 
- Python
- Sklearn
- 菜菜
- 机器学习
toc: true
mathjax: true
mathjax_autoNumber: false
key: 20220214
---

## 概述

### 决策什么是决策树

决策树（Decision Tree）是一种非参数的**有监督**学习方法，它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。

<div align="center">
	<img src="https://raw.githubusercontent.com/cocotwp/cocotwp.github.io/master/assets/images/决策树/决策树样例.png" alt="决策树样例" width="60%" />
</div>

决策树算法的两个核心问题：
1. 如何从数据表中找出最佳节点和最佳分枝？
2. 如何让决策树停止胜场，防止过拟合？

### sklearn 中的决策树

`sklearn.tree` 模块包含以下类：

| tree.BaseDecisionTree | 基本决策树 |
| tree.DecisionTreeClassifier | 分类树 |
| tree.DecisionTreeRegressor | 回归树 |
| tree.ExtraTreeClassifier | 高随机版本的分类树 |
| tree.ExtraTreeRegressor | 高随机版本的回归树 | 
| tree.export_text 、 tree.export_graphviz 、 tree.plot_tree | 绘图 |

**基本建模流程**

```python
from sklearn import tree

# 1. 实例化模型对象
clf = tree.DecisionTreeClassifier()
# 2. 用训练集数据训练模型
clf = clf.fit(x_train, y_train)
# 3. 验证测试集
result = clf.score(x_test, y_test)
```

## DecisionTreeClassifier

_class_ sklearn.tree.DecisionTreeClassifier(_*_, _criterion='gini'_, _splitter='best'_, _max_depth=None_, _min_samples_split=2_, _min_samples_leaf=1_, _min_weight_fraction_leaf=0.0_, _max_features=None_, _random_state=None_, _max_leaf_nodes=None_, _min_impurity_decrease=0.0_, _class_weight=None_, _ccp_alpha=0.0_)

### 重要参数

#### criterion

`criterion:{"gini", "entropy"}, default="gini"`

为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标叫做“不纯度”。现在使用的决策树算法在分枝方法上的核心大多是围绕在某个不纯度相关指标的最优化上。

sklearn 提供了两种选择：**信息熵**（Entropy）、**基尼系数**（Gini Impurity）

$$Entropy(t)=-\sum_{i=0}^{c-1}p(i|t)log_2p(i|t)$$

$$Gini(t)=1-\sum_{i=0}^{c-1}p(i|t)^2$$

其中 $$t$$ 代表给定的节点，$$i$$ 代表标签的任意分类，$$p(i|t)$$ 代表标签分类 $$i$$ 在节点 $$t$$ 上所占的比例。**注意**，当使用信息熵时，sklearn 实际计算的是基于信息熵的信息增益（Information Gain），即父节点的信息熵和子节点的信息熵之差。

比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是*在实际使用中，信息熵和基尼系数的效果基本相同*。

#### random_state

`random_state{int, RandomState instance or None}, default=None`

`random_state` 用来设置分枝中的随机模式的参数，默认 None，对高维度数据随机性会表现更明显，而对低维度数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。

#### splitter

`splitter:{"best", "random"}, default="best"`

`splitter` 也是用来控制决策树中的随机选项的，有两种选项，选择“best”时，决策树在分枝时虽然随机，但是还是会优先选择更重要的的特征进行分枝；选择“randon”时，决策树在分枝时会更加随机。\
树会因为含有更多的不必要信息而降低对训练集的拟合，这也是防止过拟合的一种方式。

#### 剪枝参数

