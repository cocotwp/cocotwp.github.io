---
layout: article
title: Spark 基础入门
tags: 
- Spark
- 黑马
toc: true
---

## 第1章 Spark 框架概述

### Spark 是什么

[Apache Spark](https://spark.apache.org) 是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习。\
Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.

### Spark 有哪些特性

- 批处理/流式数据（Batch/streaming data）
- SQL分析（SQL analytics）
- 弹性数据科学（Data science at scale）
- 机器学习（Machine learning）

### Spark 有哪些模块

核心SparkCore、SQL计算（SparkSQL）、流计算（SparkStreaming）、图计算（GraphX）、机器学习（MLib）

### Spark 的运行模式

- 本地模式（Local）
- 集群模式（StandAlone、YARN、K8S）
- 云模式

### Spark 的运行角色（对比 YARN）

- Master：集群资源管理（类同 ResourceManager）
- Worker：单机资源管理（类同 NodeManager）
- Driver：单任务管理者（类同 ApplicationMaster）
- Executor：单任务执行者（类同 YARN 容器内的 Task）

## Spark 环境搭建-Local

**略**

## Spark 环境搭建-Local

### Standalone 架构

*Standalone集群* 在晋城上主要有3类进程：

- 主节点 Master：
管理整个集群资源各个任务的 Driver；
- 从节点 Workers：
管理每个机器的资源，分配对应的资源来运行 Executor（Task）；
- 历史服务器 HistoryServer（可选）：
Spark Application 运行完成以后，保存事件日志数据至 HDFS，启动 HistoryServer 可以查看应用运行相关信息。

### Standalone 环境安装操作

**略**

### Spark 应用架构

![spark执行阶段](https://raw.githubusercontent.com/cocotwp/cocotwp.github.io/master/assets/images/Spark执行阶段.png)

用户应用程序从最开始的提交到最终的计算执行，需要经历一下几个阶段：
1. 用户程序创建 `SparkContext` 时，新创建的 `SparkContext` 实例会连接到 `ClusterManager`。`Cluster Manager` 会根据用户提交时设置的 CPU 和内存等信息为本次提交分配计算资源，启动 `Executor`；
2. `Driver` 会将用户程序划分为不同的执行阶段 Stage，每个执行阶段 Stage 由一组完全相同 Task 组成，这些 Task 分别作用于待处理数据的不同分区。在阶段划分完成和 Task 创建后， `Driver` 会向 `Executor` 发送 Task；
3. `Executor` 在接收到 Task 后，会下载 Task 的运行时依赖，在准备好 Task 的执行环境后，会开始执行 Task，并且将 Task 的运行状态汇报给 `Driver`；
4. `Driver` 会根据收到的 Task 的运行状态来处理不同的状态更新。Task 分为两种：一种是 ==Shuffle Map Task==，它实现数据的重新洗牌，洗牌的结果保存到 `Executor` 所在节点的文件系统中；另外一种是 ==Result Task==，它负责生成结果数据；
5. `Driver` 会不断地调用 Task，将Task发送到 `Executor` 执行，在所有的Task 都正确执行或者超过执行次数的限制仍然没有执行成功时停止。