---
layout: article
title: 特征工程
tags: 
- Python
- Sklearn
- 菜菜
- 机器学习
toc: true
mathjax: true
mathjax_autoNumber: false
key: 20220412
---

## 特征选择 feature_selection

- 特征选择（feature extraction）：从文字、图像、声音等其他非结构化数据中提取新信息作为特征
- 特征创造（feature creation）：把现有特征进行组合，或相互计算，得到新特征
- 特征选择（feature selection）：从所有特征中，选择出有意义、对模型有帮助的特征，以避免必须将所有特征都导入模型去训练的情况。

```python
import pandas as pd

file_path = '/Users/coco/Downloads/【机器学习】菜菜的sklearn课堂(1-12全课)/03数据预处理和特征工程/digit recognizor.csv')

data = pd.read_csv(file_path)
data.head()

X = data.iloc[:, 1:]
y = data.iloc[:, 0]

X.shape		# (42000, 784)
```

### Filter 过滤法

过滤方法通常用作预处理步骤，特征选择完全独立于任何机器学习算法。它是根据各种统计验证中的分数以及相关性的各项指标来选择特征。

#### 方差过滤

##### VarianceThreshold

如果一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，，可能能特征中的大多数值都一样，尤其是整个特征的取值都一样，那么这个特征对于样本区分没有任何作用。

```python
from sklearn.feature_selection import VarianceThreshold

X_var0 = VarianceThreshold().fit_transform(X)  # 阈值默认为0
X_var0.shape  # (42000, 708)
```

将特征方差的中位数作为阈值，可以让特征数量减半。

```python
import numpy as np

X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)
X_fsvar.shape  # (42000, 392)
```

##### 方差过滤对模型的影响

**分别使用 X 和 X_fsvar 运行 KNN 模型**

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

cross_val_score(KNeighborsClassifier(), X, y, cv=5).mean()	
# 0.965857142857143

%%timeit
cross_val_score(KNeighborsClassifier(), X, y, cv=5).mean()
# 1 loop, best of 5: 1min 33s per loop
```

```python
cross_val_score(KNeighborsClassifier(), X_fsvar, y, cv=5).mean()
# 0.966

%%timeit
cross_val_score(KNeighborsClassifier(), X_fsvar, y, cv=5).mean()
# 1 loop, best of 5: 59.4 s per loop
```

可以看到，方差过滤后，准确率稍有提升，但运行效率显著提升，运行时间减少大约1/3。

**分别使用 X 和 X_fsvar 运行随机森林**

```python
from sklearn.neighbors import RandomForestClassifier
from sklearn.model_selection import cross_val_score

cross_val_score(RandomForestClassifier(n_estimators=10, random_state=0), X, y, cv=5).mean()
# 0.9373571428571429

%%timeit
cross_val_score(RandomForestClassifier(n_estimators=10, random_state=0), X, y, cv=5).mean()
# 1 loop, best of 5: 13.6 s per loop
```

```python
cross_val_score(RandomForestClassifier(n_estimators=10, random_state=0), X_fsvar, y, cv=5).mean()
# 0.9390476190476191

%%timeit
cross_val_score(RandomForestClassifier(n_estimators=10, random_state=0), X_fsvar, y, cv=5).mean()
# 1 loop, best of 5: 13.3 s per loop
```

可以看到，方差过滤后，随机森林的准确率有所上升，但运行时间几乎没有变化。

**为什么随机森林运行如此之快？为什么方差过滤对随机森林没有很大的影响？**

这是由于两种算法的原理中涉及到的计算量不同。
最近邻算法KNN、单棵决策树、支持向量机SVM、神经网络、回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平。这其实很容易理解，无论过滤法如何降低特征的数量，随机森林也只会选取固定数量的特征来建模；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。
因此，过滤法的**主要对象**是：**需要遍历特征或升维的算法们**，而过滤法的**主要目的**是：**在维持算法表现的前提下，帮助算法们降低计算成本**。

##### 超参 threshold

现实中，我们只会使用阈值为0或者阈值很小的方差过滤，来为我们优先消除一些明显用不到的特征，然后我们会选择更优的特征选择方法继续减少特征数量。

#### 相关性过滤

选出与标签有关且有意义的特征

##### 卡方过滤

卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。