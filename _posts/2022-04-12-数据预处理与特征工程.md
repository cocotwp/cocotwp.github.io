---
layout: article
title: 特征工程
tags: 
- Python
- Sklearn
- 菜菜
- 机器学习
toc: true
mathjax: true
mathjax_autoNumber: false
key: 20220412
---

## 特征选择 feature_selection

- 特征选择（feature extraction）：从文字、图像、声音等其他非结构化数据中提取新信息作为特征
- 特征创造（feature creation）：把现有特征进行组合，或相互计算，得到新特征
- 特征选择（feature selection）：从所有特征中，选择出有意义、对模型有帮助的特征，以避免必须将所有特征都导入模型去训练的情况。

```python
import pandas as pd

file_path = '/Users/coco/Downloads/【机器学习】菜菜的sklearn课堂(1-12全课)/03数据预处理和特征工程/digit recognizor.csv')

data = pd.read_csv(file_path)
data.head()

X = data.iloc[:, 1:]
y = data.iloc[:, 0]

X.shape		# (42000, 784)
```

### Filter 过滤法

过滤方法通常用作预处理步骤，特征选择完全独立于任何机器学习算法。它是根据各种统计验证中的分数以及相关性的各项指标来选择特征。

#### 方差过滤

##### VarianceThreshold

如果一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，，可能能特征中的大多数值都一样，尤其是整个特征的取值都一样，那么这个特征对于样本区分没有任何作用。

```python
from sklearn.feature_selection import VarianceThreshold

X_var0 = VarianceThreshold().fit_transform(X)  # 阈值默认为0
X_var0.shape  # (42000, 708)
```

将特征方差的中位数作为阈值，可以让特征数量减半。

```python
import numpy as np

X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)
X_fsvar.shape  # (42000, 392)
```

##### 方差过滤对模型的影响

**分别使用 X 和 X_fsvar 运行 KNN 模型**

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

cross_val_score(KNeighborsClassifier(), X, y, cv=5).mean()	
# 0.965857142857143

%%timeit
cross_val_score(KNeighborsClassifier(), X, y, cv=5).mean()
# 1 loop, best of 5: 1min 33s per loop
```

```python
cross_val_score(KNeighborsClassifier(), X_fsvar, y, cv=5).mean()
# 0.966

%%timeit
cross_val_score(KNeighborsClassifier(), X_fsvar, y, cv=5).mean()
# 1 loop, best of 5: 59.4 s per loop
```

可以看到，方差过滤后，准确率稍有提升，但运行效率显著提升，运行时间减少大约1/3。

**分别使用 X 和 X_fsvar 运行随机森林**

```python
from sklearn.neighbors import RandomForestClassifier
from sklearn.model_selection import cross_val_score

cross_val_score(RandomForestClassifier(n_estimators=10, random_state=0), X, y, cv=5).mean()
# 0.9373571428571429

%%timeit
cross_val_score(RandomForestClassifier(n_estimators=10, random_state=0), X, y, cv=5).mean()
# 1 loop, best of 5: 13.6 s per loop
```

```python
cross_val_score(RandomForestClassifier(n_estimators=10, random_state=0), X_fsvar, y, cv=5).mean()
# 0.9390476190476191

%%timeit
cross_val_score(RandomForestClassifier(n_estimators=10, random_state=0), X_fsvar, y, cv=5).mean()
# 1 loop, best of 5: 13.3 s per loop
```

可以看到，方差过滤后，随机森林的准确率有所上升，但运行时间几乎没有变化。

**为什么随机森林运行如此之快？为什么方差过滤对随机森林没有很大的影响？**

这是由于两种算法的原理中涉及到的计算量不同。
最近邻算法KNN、单棵决策树、支持向量机SVM、神经网络、回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平。这其实很容易理解，无论过滤法如何降低特征的数量，随机森林也只会选取固定数量的特征来建模；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。
因此，过滤法的**主要对象**是：**需要遍历特征或升维的算法们**，而过滤法的**主要目的**是：**在维持算法表现的前提下，帮助算法们降低计算成本**。

##### 超参 threshold

现实中，我们只会使用阈值为0或者阈值很小的方差过滤，来为我们优先消除一些明显用不到的特征，然后我们会选择更优的特征选择方法继续减少特征数量。

#### 相关性过滤

选出与标签有关且有意义的特征

##### 卡方过滤

卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。

```python
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

X_fschi = SelectKBest(chi2, k=300).fit_transform(X_fsvar, y) # 保留300个特征
X_fschi.shape  #(42000, 300)

cross_val_score(KNeighborsClassifier(), X_fschi, y, cv=5).mean()
# 0.9606190476190477
```

可以看出，模型的准确率下降了，这说明 `k=300` 设置的过小了，删除了与模型有关且有效的特征。如果模型的表现提升，则说明我们的相关性过滤是有效的，过滤掉了模型的噪声，这是应该保留过滤后的结果。

##### 选取超参 k

**学习曲线**

```python
import matplotlib.pyplot as plt

scores = []

for i in range(200, 390, 10):
	X_fschi = SelectKBest(chi2, k=i).fit_transform(X_fsvar, y)
	score = cross_val_score(KNeighborsClassifier(), X_fschi, y, cv=5).mean()
	scores.append(score)

plt.plot(range(200, 390, 10), scores)
plt.show()
```

<div align="center">
	<img data="https://raw.githubusercontent.com/cocotwp/cocotwp.github.io/master/assets/images/数据预处理与特征工程/过滤法-卡方过滤.png" width="80%" />
</div>
 
 可以看到，随着 K 值的增加，模型效果不断增加，这说明，所有特征对模型都是有效的。
 
 但是运行学习曲线的时间也是非常长，接下来我们介绍一种更好的方法：看 p 值选 k
 
 卡方检验的本质是推测两组数据之间的差异，其检验的原假设是**“两组数据是相互独立的”**。卡方检验返回卡方值和 p 值两个统计量，其中卡方值很难界定有效的范围，而 p 值，我们一般使用0.01或0.05作为显著性水平。
 
 | p值 | <=0.05或0.01 | >0.05或0.01 |
 | 数据差异 | 差异不是自然形成的 | 差异是自然的样本误差 |
 | 相关性 | 两组数据是相关的 | 两组数据是相互独立的 |
 | 原假设 | 拒绝原假设 | 接受原假设 |
 
 ```python
chivalue, pvalues_chi = chi2(X_fsvar, y)

chivalue

pvalues_chi

# k取值
k = pvalues_chi.shape[0] - (pvalues_chi > 0.05).sum()  # 392
```

可以看到，所有特征的 p 值都是0，这说明所有特征都与标签相关。在这种情况下，舍弃任何一个特征，都会舍弃对模型有用的信息，而使模型表现下降。

##### F 检验

F 检验，又称 ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法，它即可以做回归也可以做分类。

F 检验的本质是寻找两组数据之间的线性关系，其原假设是**”数据不存在显著的线性关系“**。它返回 F 值和 p 值两个统计量。

```python
from sklearn.feature_selection import f_classif

F, pvalues_f = f_classif(X_fsvar, y)

F

pvalues_f

# k取值
k = pvalues_f.shape[0] - (pvalues_f > 0.05).sum()  # 392
```

得到的结论和[[#卡方过滤]]的一样：没有任何特征的 p 值大于0.01，所有的特征都是和标签相关的，因此我们不需要相关性过滤。

#### 互信息法

