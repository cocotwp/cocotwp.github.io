---
layout: article
title: Spark SQL
tags: 
- Spark
- 黑马
toc: true
---

## 快速入门

### 什么是 SparkSQL

Spark SQL is Spark's module for working with structured data.

*限定*：结构化数据处理

### SparkSQL 的特点

1. 融合性：SQL 可以无缝集成在代码中，随时用 SQL 处理数据
2. 统一数据访问：一套标准 API 可读写不同数据源
3. Hive 兼容：可以使用 SparkSQL 直接计算并生成 Hive 数据表
4. 标准化连接：支持标准化 JDBC\\ODBC 连接，方便和各种数据库进行数据交互

## SparkSQL 概述

### SparkSQL 和 Hive 的异同

<div align="center">
	<img src="https://raw.githubusercontent.com/cocotwp/cocotwp.github.io/master/assets/images/sparkSQL/SparkSQL和Hive的异同.png" alt="SparkSQL和Hive的异同" width="75%" />
</div>

SparkSQL 和 Hive 都是”分布式 SQL 计算引擎“

### SparkSQL 的数据抽象

| Pandas - DataFrame | SparkCore - RDD | SparkSQL - DataFrame |
| :---: | :---: | :---: |
| <font color="red">二维表数据结构</font> | 不限定标准数据结构 | <font color="red">二维表数据结构</font> |
| 本地集合 | <font color="red">分布式集合</font> | <font color="red">分布式集合</font> |

SparkSQL 其实有3类数据抽象对象：
- SchemaRDD 对象（已废弃）：引入 Schema 的 RDD
- Dataset 对象：加入泛型特性，可用于 Java、Scala 语言
- DataFrame 对象：可用于 Java、Scala、Python、R

### DataFrame 概述

DataFrame v.s. RDD：

| | DataFrame | RDD |
| --- | :---: | :---: |
| 存储能力 | 只能存储二维表结构数据 | 存储任意结构的数据 |
| 示例 | <img src="https://raw.githubusercontent.com/cocotwp/cocotwp.github.io/master/assets/images/sparkSQL/DFvsRDD1.png" alt="DFvsRDD1" height="100px" /> | <img src="https://raw.githubusercontent.com/cocotwp/cocotwp.github.io/master/assets/images/sparkSQL/DFvsRDD2.png" alt="DFvsRDD2" height="100px" /> |

### SparkSession 对象

在 Spark 2.0 后，推出了 `SparkSession` 对象，作为 Spark 编程的统一入口对象。

SparkSession 对象可以：
- 用于 SparkSQL 编程作为入口对象
- 用于 SparkCore 编程，可以通过 SparkSession 对象中获取到 SparkContext

```python
from pyspark.sql import SparkSession

if __name__ == '__main__':
	spark = SparkSession.builder.\
		appName('myApp').\
		master('local').\
		config(...).\
		getOrCreate()
```

## DataFrame 入门

### DataFrame 的组成

DataFrame 是一个二维表结构，其表格结构（表、行、列）的组成如下：

在结构层面：
- StructType 对象描述整个 DataFrame 的表结构
- StructField 对象描述一个列的信息

在数据层面：
- Row 对象记录一行数据
- Column 对象记录一列数据并包含列的信息

StructType 的构建方法如下：
```python
struct_type = StructType().\
	add("id", IntegerType(), False).\
	add("subject", StringType(), True).\
	add("age", IntegerType(), False)
```

- StructField 包含：列名、列类型、是否可为空
- 一个 StructType 对象由多个 StructField 组成

### 基于 RDD 构建

DataFrame 对象可以从 RDD 转换而来

#### 使用列表指定列名，列类型推断得到

```python
# coding:utf8
from pyspark.sql import SparkSession

if __name__ == '__main__':
    spark = SparkSession.builder.\
        appName("myApp").\
        master("local").\
        getOrCreate()
    sc = spark.sparkContext

    rdd = sc.textFile("./data/input/sql/people.txt").\
        map(lambda x: x.split(",")).\
        map(lambda x: (x[0], int(x[1])))

    df = spark.createDataFrame(rdd, schema=['name','age'])
    df.printSchema()
    df.show()
```

#### 使用 StructType 对象定义表结构

```python
    schema = StructType().add("name", StringType(), False).\
        add("age", IntegerType(), True)
    df = spark.createDataFrame(rdd, schema=schema)
```

#### 使用 rdd.toDF() 方法

```python
	df = rdd.toDF(["name", "age"])
```

```python
	df = rdd.toDF(schema=schema)
```

### 基于 Pandas 的 DataFrame

```python
	df = spark.createDataFrame(pandas_df, schema=...)
```

### 读取外部数据

通过 SparkSQL 的**统一 API** 进行数据读取构建 DataFrame

```python
spark.read.format("text|csv|json|parquet|orc|avro|jdbc|...")
	.option("Key", "Value")  # 可选
	.schema(StructType|String)
	.load("/path/to/file")  # 支持本地文件和HDFS
```

- *format("text")*：只有一个列，列明默认为 value
- *format("json")*：一般不用指明 schema，json 自带列名和列类型（字符串和数字）
- *format("csc")*
- *format("parquet")*：一种列存储文件格式，内置 schema，经过序列化
- ...

### DataFrame 的入门操作

DataFrame 支持两种编程风格：
- DSL 风格
- SQL 风格

#### DSL 风格

- *show*：展示 DataFrame 中的数据，默认展示20条
- *printSchema*：打印输出 df 的 schema 信息
- *select*：选择 DataFrame 中的指定列
- *where(filter)*：过滤 DataFrame 内的数据，返回一个过滤后的 DataFrame
- *groupBy*：按照指定的列进行数据分组，返回值是 `GroupedData` 对象

#### SQL 风格

如果想要使用 SQL 风格的语法，需要将 DataFrame 注册成表，采用如下的方式之一：
- df.createTempView("table_name")  # 注册一个临时视图
- df.createOrReplace("table_name")  # 注册一个临时试图，如果存在则进行替换
- df.createGlobalTempView("table_name")  # 注册一个全局视图
- df.createOrReplaceGlobalTempView("table_name")  # 注册一个全局视图，如果存在则进行替换

**临时表和全局表**
- 临时表：只在当前 SparkSession 中可用
- 全局表：跨 SparkSession 对象使用，在一个程序内的多个 SparkSession 中均可调用，查询前带上前缀 `global_temp`

表注册完成后，可以通过以下语法进行查询：
```python
spark.sql("""
	sql_query
""").show()
```

#### pyspark.sql.functions 包

PySpark 提供了一个包：`pyspark.sql.functions`\
包内提供了一系列的计算函数，供 SparkSQL 使用

### WordCount 案例练习

```python
# coding:utf8
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode

if __name__ == '__main__':
    spark = SparkSession.builder.\
        appName("myApp").\
        master("local").\
        getOrCreate()
    sc = spark.sparkContext

	# SQL
    rdd = sc.textFile("./data/input/words.txt").\
        flatMap(lambda x: x.split(" ")).\
        map(lambda x: [x])
    df = rdd.toDF(["word"])
    df.createOrReplaceTempView("words")

    spark.sql("""
        select word, count(*)
        from words
        group by word
    """).show()

	# DSL
    df = spark.read.format("text").\
        schema("word string").\
        load("./data/input/words.txt")
    df.select(explode(split("word", " ")).alias("word")).\
        groupBy("word").\
        count().\
        show()
```

### 电影评分数据分析案例

```python
# coding:utf8
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StringType, IntegerType
from pyspark.sql.functions import avg, count

if __name__ == '__main__':
    spark = SparkSession.builder.\
        appName("myApp").\
        master("local").\
        getOrCreate()
    sc = spark.sparkContext

    schema = StructType().add("user_id", StringType(), False).\
        add("movie_id", StringType(), False).\
        add("score", IntegerType(), False).\
        add("ts", StringType(), True)
    df = spark.read.format("csv").\
        option("sep", "\t").\
        option("header", False).\
        option("encoding", "utf-8").\
        schema(schema).\
        load("./data/input/sql/u.data")

    # 1. 求用户平均分
    df.groupBy("user_id").\
        avg("score").\
        show()

    # 2. 求电影平均分
    df.groupBy("movie_id").\
        avg("score").\
        show()

    # 3. 大于平均分的评分的数量
    score_avg = df.select(avg("score")).first()[0]
    print(df.where(f"score > {score_avg}").count())

    # 4. 找出打分次数最多的用户，并计算它的打分平均分
    max_user_id = df.groupBy("user_id").\
        count().\
        orderBy("count", ascending=False).\
        first()["user_id"]
    # print(max_user_id)
    print(df.where(f"user_id={max_user_id}").select(avg("score")).first()[0])

    # 5. 查询评分次数超过100次的电影，按平均分展示TOP10
    df.groupBy("movie_id").\
        agg(
            count("score").alias("cnt"),
            avg("score").alias("score_avg")).\
        where("cnt>100").\
        orderBy("score_avg", ascending=False).\
        show(10)
```

### SparkSQL Shuffle 分区数据

原因：在 SparkSQL 中当 Job 中产生 Shuffle 时，默认的分区数（spark.sql.shuffle.partitions）为200，在实际项目中要合理的设置。

可以设置在：
1. 配置文件：`conf/spark-defaults.conf` - `spark.sql.shuffle.partitions 2`
2. 在客户端提交参数中：`bin/spark-submit --conf "spark.sql.shuffle.partitions=2"`
3. 在代码中可以设置：
```python
spark = SparkSession.builder.\
	appName("myApp").\
	master("local").\
	config("spark.sql.shuffle.partitions", "2").\
	getOrCreate()
```

### SparkSQL 数据清洗 API



## SparkSQL 函数定义

## SparkSQL 的运行流程

## SparkSQL 整合 Hive

## 分布式 SQL 引擎配置

